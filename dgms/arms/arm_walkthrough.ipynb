{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Walkthrough of Creating a Basic ARM\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# Digits dataset\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn import datasets\n",
    "# Pytorch\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from pytorch_model_summary import summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset\n",
    "\n",
    "**Scipy Digits:**\n",
    "- $N \\approx 1500$\n",
    "- Each image is $8 \\times 8$\n",
    "- $\\mathcal{X} = \\{0,1,2, \\dots 16\\}$\n",
    "\n",
    "Defining the dataset class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Digits(Dataset):\n",
    "    \"\"\"\n",
    "    SciKit Learn Digits dataset wrapper for PyTorch\n",
    "    \"\"\"\n",
    "    def __init__(self, mode='train', transforms=None):\n",
    "        \n",
    "        self.digits = load_digits()\n",
    "        \n",
    "        if mode=='train':\n",
    "            self.data = self.digits.data[:1000].astype(np.float32)\n",
    "        elif mode=='val':\n",
    "            self.data = self.digits.data[1000:1350].astype(np.float32)\n",
    "        else:\n",
    "            self.data = self.digits.data[1350:].astype(np.float32)\n",
    "        \n",
    "        \n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.data[idx]\n",
    "        if self.transforms:\n",
    "            sample = self.transforms(sample)\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the ARM\n",
    "#### Causal Convolutional Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CausalConv1d(nn.Module):\n",
    "    \"\"\"\n",
    "    Causal 1D convolution layer\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            in_channels,\n",
    "            out_channels,\n",
    "            kernel_size,\n",
    "            dilation, # Dilation factor\n",
    "            A=False, # NOTE: Determines whether to use \"A\" mode or not\n",
    "            **kwargs\n",
    "    ):\n",
    "        super(CausalConv1d, self).__init__()\n",
    "        \n",
    "        # Basic attributes\n",
    "        self.A = A\n",
    "        self.kernel_size = kernel_size \n",
    "        self.padding = (kernel_size - 1) * dilation + A*1 # NOTE: 1 added for \"A\" mode (initial layer)\n",
    "        self.dilation = dilation\n",
    "\n",
    "\n",
    "        self.conv1d = nn.Conv1d(\n",
    "            in_channels,\n",
    "            out_channels,\n",
    "            kernel_size,\n",
    "            stride=1,\n",
    "            padding=0,\n",
    "            dilation=dilation,\n",
    "            **kwargs \n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = nn.functional.pad(x, (self.padding, 0))\n",
    "        conv1d_out = self.conv1d(x)\n",
    "        \n",
    "        if self.A:\n",
    "            return conv1d_out[:, :, :-1]\n",
    "        else:\n",
    "            return conv1d_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Log Categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPS = 1.e-5\n",
    "\n",
    "def log_categorical(\n",
    "    x, \n",
    "    p, \n",
    "    num_classes=256, # Number of pixel values\n",
    "    reduction=None, \n",
    "    dim=None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Log categorical distribution\n",
    "    \"\"\"\n",
    "    x_one_hot = F.one_hot(x.long(), num_classes=num_classes) # Basically implements the iverson bracket\n",
    "    log_p = x_one_hot * torch.log(torch.clamp(p, EPS, 1. - EPS))\n",
    "    if reduction == 'sum':\n",
    "        return torch.sum(log_p, dim)\n",
    "    elif reduction == 'avg':\n",
    "        return torch.mean(log_p, dim)\n",
    "    else:\n",
    "        return log_p \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ARM!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ARM(nn.Module):\n",
    "    def __init__(self, net, D=2, num_vals=256):\n",
    "        super(ARM, self).__init__()\n",
    "        self.net = net\n",
    "        self.D = D\n",
    "        self.num_vals = num_vals\n",
    "        \n",
    "    def f(self, x):\n",
    "        \n",
    "        h = self.net(x.unsqueeze(1))\n",
    "        h = h.permute(0, 2, 1)\n",
    "        p = torch.softmax(h, 2)\n",
    "        \n",
    "        return p\n",
    "\n",
    "    def forward(self, x, reduction='avg'):\n",
    "        if reduction == 'avg':\n",
    "            return -(self.log_prob(x).mean())\n",
    "        elif reduction == 'sum':\n",
    "            return -(self.log_prob(x).sum())\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "        \n",
    "    def log_prob(self, x):\n",
    "        mu_d = self.f(x)\n",
    "        log_p = log_categorical(x, mu_d, num_classes=self.num_vals, reduction='sum', dim=2).sum(-1)\n",
    "\n",
    "        return log_p\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        x_new = torch.zeros(batch_size, self.D)\n",
    "        for d in range(self.D):\n",
    "            p = self.f(x_new)\n",
    "            x_new_d = torch.multinomial(p[:, d, :], num_samples=1)\n",
    "            x_new[:, d] = x_new_d.squeeze() # NOTE: Squeeze to remove the last dimension\n",
    "            \n",
    "        return x_new\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Auxiliary Functions\n",
    "\n",
    "#### Training, Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation(\n",
    "        val_loader,\n",
    "        name = None,\n",
    "        model_best = None,\n",
    "        epoch = None,\n",
    "):\n",
    "    if model_best is None:\n",
    "        print(f\"Loading model from: {name + '.model'}\")\n",
    "        model_best = torch.load(name + '.model')\n",
    "    \n",
    "    loss = 0\n",
    "    N = 0\n",
    "\n",
    "    for _, test_batch in enumerate(val_loader):\n",
    "        \n",
    "        loss += model_best.forward(test_batch, reduction='sum').item()\n",
    "        N += test_batch.size(0)\n",
    "\n",
    "    loss /= N\n",
    "\n",
    "    if epoch is None:\n",
    "        print('Test Loss: {:.4f}'.format(loss))\n",
    "    else:\n",
    "        print('Epoch: {}, Val Loss: {:.4f}'.format(epoch, loss))\n",
    "    \n",
    "    return loss\n",
    "\n",
    "def samples_real(name, test_loader):\n",
    "\n",
    "    num_x, num_y = 4, 4\n",
    "    x = next(iter(test_loader)).detach().numpy()\n",
    "\n",
    "    fig, ax = plt.subplots(num_x, num_y, figsize=(10, 10))\n",
    "    for i, ax in enumerate(ax.flatten()):\n",
    "        \n",
    "        ax.imshow(x[i].reshape(8, 8), cmap='gray')\n",
    "        ax.axis('off')\n",
    "    \n",
    "    plt.savefig(name + '_real_images.png', bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "def samples_generated(name, data_loader, extra_name=''):\n",
    "    \n",
    "    x = next(iter(data_loader)).detach().numpy()\n",
    "\n",
    "    # Generate\n",
    "    model_best = torch.load(name + '.model', weights_only=False)\n",
    "    model_best.eval()\n",
    "\n",
    "    num_x, num_y = 4, 4\n",
    "    x = model_best.sample(num_x*num_y).detach().numpy()\n",
    "\n",
    "    _, ax = plt.subplots(num_x, num_y, figsize=(10, 10))\n",
    "    for i, ax in enumerate(ax.flatten()):\n",
    "        \n",
    "        ax.imshow(x[i].reshape(8, 8), cmap='gray')\n",
    "        ax.axis('off')\n",
    "\n",
    "    plt.savefig(name + '_generated_images' + extra_name + '.png', bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "def plot_curve(name, train_loss, val_loss):\n",
    "    \n",
    "    fig, ax = plt.subplots(1, 1, figsize=(10, 5))\n",
    "    ax.plot(train_loss, label='Train')\n",
    "    ax.plot(val_loss, label='Val')\n",
    "    ax.set_xlabel('Epoch')\n",
    "    ax.set_ylabel('Loss')\n",
    "    ax.legend()\n",
    "    plt.savefig(name + '_loss_curve.png', bbox_inches='tight')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(\n",
    "        name, \n",
    "        max_patience,\n",
    "        num_epochs,\n",
    "        model,\n",
    "        optimizer,\n",
    "        train_loader,\n",
    "        val_loader,\n",
    "):\n",
    "    val_loss = []\n",
    "    train_loss = []\n",
    "    best_val_loss = 1000\n",
    "    patience = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "\n",
    "        # Training\n",
    "        \n",
    "        model.train()\n",
    "        train_loss_ep = 0\n",
    "        N = 0\n",
    "\n",
    "        for _, train_batch in enumerate(train_loader):\n",
    "\n",
    "            if hasattr(model, 'dequantization'):\n",
    "                if model.dequantization:\n",
    "                    batch = batch + torch.rand_like(batch)\n",
    "                \n",
    "\n",
    "            loss = model.forward(train_batch)            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward(retain_graph=True)\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss_ep += loss.item()\n",
    "            N += train_batch.size(0)\n",
    "\n",
    "\n",
    "        train_loss_ep /= N\n",
    "        train_loss.append(train_loss_ep)\n",
    "\n",
    "        # Validation\n",
    "        val_loss_ep = evaluation(\n",
    "            val_loader,\n",
    "            name=name,\n",
    "            model_best=model,\n",
    "            epoch=epoch\n",
    "        )\n",
    "        val_loss.append(val_loss_ep)\n",
    "        print('Epoch: {}, Train Loss: {:.4f}, Val Loss: {:.4f}'.format(epoch, train_loss_ep, val_loss_ep))\n",
    "\n",
    "        if epoch == 0:\n",
    "            print('saved')\n",
    "            torch.save(model, name + '.model')\n",
    "            best_val_loss = val_loss_ep\n",
    "        else:  \n",
    "            if val_loss_ep < best_val_loss:\n",
    "                print('saved')\n",
    "                best_val_loss = val_loss_ep\n",
    "                patience = 0\n",
    "                torch.save(model, name + '.model')\n",
    "                samples_generated(name, val_loader, extra_name='_epoch_' + str(epoch))\n",
    "            else:\n",
    "                patience += 1\n",
    "\n",
    "        if patience > max_patience:\n",
    "            break\n",
    "\n",
    "    val_loss = np.array(val_loss)\n",
    "    train_loss = np.array(train_loss)\n",
    "    \n",
    "    return train_loss, val_loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = Digits(mode='train')\n",
    "val_data = Digits(mode='val')\n",
    "test_data = Digits(mode='test')\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_data, batch_size=64, shuffle=False)\n",
    "test_loader = DataLoader(test_data, batch_size=64, shuffle=False)\n",
    "\n",
    "results_dir = 'results/'\n",
    "os.makedirs(results_dir, exist_ok=True)\n",
    "name = 'arm'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.Digits at 0x1203e7cb0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = 64\n",
    "M = 256\n",
    "\n",
    "lr = 1e-3\n",
    "num_epochs = 100\n",
    "max_patience = 20\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize ARM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------\n",
      "      Layer (type)        Output Shape         Param #     Tr. Param #\n",
      "=======================================================================\n",
      "    CausalConv1d-1        [1, 256, 64]           2,048           2,048\n",
      "       LeakyReLU-2        [1, 256, 64]               0               0\n",
      "    CausalConv1d-3        [1, 256, 64]         459,008         459,008\n",
      "       LeakyReLU-4        [1, 256, 64]               0               0\n",
      "    CausalConv1d-5        [1, 256, 64]         459,008         459,008\n",
      "       LeakyReLU-6        [1, 256, 64]               0               0\n",
      "    CausalConv1d-7         [1, 17, 64]          30,481          30,481\n",
      "=======================================================================\n",
      "Total params: 950,545\n",
      "Trainable params: 950,545\n",
      "Non-trainable params: 0\n",
      "-----------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "likelihood_type = 'categorical'\n",
    "num_classes = 17\n",
    "kernel = 7\n",
    "\n",
    "net = nn.Sequential(\n",
    "    CausalConv1d(in_channels=1, out_channels=M, dilation=1, kernel_size=kernel, A=True, bias=True), #Â <- NOTE: A=True for input layer\n",
    "    nn.LeakyReLU(),\n",
    "    CausalConv1d(in_channels=M, out_channels=M, dilation=1, kernel_size=kernel, A=False, bias=True),\n",
    "    nn.LeakyReLU(),\n",
    "    CausalConv1d(in_channels=M, out_channels=M, dilation=1, kernel_size=kernel, A=False, bias=True),\n",
    "    nn.LeakyReLU(),\n",
    "    CausalConv1d(in_channels=M, out_channels=num_classes, dilation=1, kernel_size=kernel, A=False, bias=True)\n",
    ")\n",
    "\n",
    "model = ARM(net, D=D, num_vals=num_classes)\n",
    "print(summary(model, torch.zeros((1, 64)), show_input=False, show_hierarchical=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adamax([p for p in model.parameters() if p.requires_grad == True], lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Val Loss: 117.9166\n",
      "Epoch: 0, Train Loss: 2.2520, Val Loss: 117.9166\n",
      "saved\n",
      "Epoch: 1, Val Loss: 112.4990\n",
      "Epoch: 1, Train Loss: 1.8165, Val Loss: 112.4990\n",
      "saved\n",
      "Epoch: 2, Val Loss: 110.3184\n",
      "Epoch: 2, Train Loss: 1.7582, Val Loss: 110.3184\n",
      "saved\n",
      "Epoch: 3, Val Loss: 108.7038\n",
      "Epoch: 3, Train Loss: 1.7247, Val Loss: 108.7038\n",
      "saved\n",
      "Epoch: 4, Val Loss: 107.3021\n",
      "Epoch: 4, Train Loss: 1.7001, Val Loss: 107.3021\n",
      "saved\n",
      "Epoch: 5, Val Loss: 105.7430\n",
      "Epoch: 5, Train Loss: 1.6746, Val Loss: 105.7430\n",
      "saved\n",
      "Epoch: 6, Val Loss: 103.9772\n",
      "Epoch: 6, Train Loss: 1.6456, Val Loss: 103.9772\n",
      "saved\n",
      "Epoch: 7, Val Loss: 102.0827\n",
      "Epoch: 7, Train Loss: 1.6142, Val Loss: 102.0827\n",
      "saved\n",
      "Epoch: 8, Val Loss: 100.2063\n",
      "Epoch: 8, Train Loss: 1.5767, Val Loss: 100.2063\n",
      "saved\n",
      "Epoch: 9, Val Loss: 98.2842\n",
      "Epoch: 9, Train Loss: 1.5448, Val Loss: 98.2842\n",
      "saved\n",
      "Epoch: 10, Val Loss: 97.1546\n",
      "Epoch: 10, Train Loss: 1.5166, Val Loss: 97.1546\n",
      "saved\n",
      "Epoch: 11, Val Loss: 96.0614\n",
      "Epoch: 11, Train Loss: 1.4926, Val Loss: 96.0614\n",
      "saved\n",
      "Epoch: 12, Val Loss: 94.9183\n",
      "Epoch: 12, Train Loss: 1.4714, Val Loss: 94.9183\n",
      "saved\n",
      "Epoch: 13, Val Loss: 94.1444\n",
      "Epoch: 13, Train Loss: 1.4544, Val Loss: 94.1444\n",
      "saved\n",
      "Epoch: 14, Val Loss: 93.6603\n",
      "Epoch: 14, Train Loss: 1.4441, Val Loss: 93.6603\n",
      "saved\n",
      "Epoch: 15, Val Loss: 93.0859\n",
      "Epoch: 15, Train Loss: 1.4335, Val Loss: 93.0859\n",
      "saved\n",
      "Epoch: 16, Val Loss: 93.3118\n",
      "Epoch: 16, Train Loss: 1.4248, Val Loss: 93.3118\n",
      "Epoch: 17, Val Loss: 92.5566\n",
      "Epoch: 17, Train Loss: 1.4160, Val Loss: 92.5566\n",
      "saved\n",
      "Epoch: 18, Val Loss: 92.5018\n",
      "Epoch: 18, Train Loss: 1.4111, Val Loss: 92.5018\n",
      "saved\n",
      "Epoch: 19, Val Loss: 91.9559\n",
      "Epoch: 19, Train Loss: 1.4044, Val Loss: 91.9559\n",
      "saved\n",
      "Epoch: 20, Val Loss: 91.8097\n",
      "Epoch: 20, Train Loss: 1.3959, Val Loss: 91.8097\n",
      "saved\n",
      "Epoch: 21, Val Loss: 91.7126\n",
      "Epoch: 21, Train Loss: 1.3883, Val Loss: 91.7126\n",
      "saved\n",
      "Epoch: 22, Val Loss: 91.4263\n",
      "Epoch: 22, Train Loss: 1.3821, Val Loss: 91.4263\n",
      "saved\n",
      "Epoch: 23, Val Loss: 91.0197\n",
      "Epoch: 23, Train Loss: 1.3764, Val Loss: 91.0197\n",
      "saved\n",
      "Epoch: 24, Val Loss: 90.7776\n",
      "Epoch: 24, Train Loss: 1.3714, Val Loss: 90.7776\n",
      "saved\n",
      "Epoch: 25, Val Loss: 90.7790\n",
      "Epoch: 25, Train Loss: 1.3650, Val Loss: 90.7790\n",
      "Epoch: 26, Val Loss: 90.8441\n",
      "Epoch: 26, Train Loss: 1.3579, Val Loss: 90.8441\n",
      "Epoch: 27, Val Loss: 90.2012\n",
      "Epoch: 27, Train Loss: 1.3549, Val Loss: 90.2012\n",
      "saved\n",
      "Epoch: 28, Val Loss: 90.2721\n",
      "Epoch: 28, Train Loss: 1.3470, Val Loss: 90.2721\n",
      "Epoch: 29, Val Loss: 90.6447\n",
      "Epoch: 29, Train Loss: 1.3446, Val Loss: 90.6447\n",
      "Epoch: 30, Val Loss: 90.5209\n",
      "Epoch: 30, Train Loss: 1.3398, Val Loss: 90.5209\n",
      "Epoch: 31, Val Loss: 90.1738\n",
      "Epoch: 31, Train Loss: 1.3367, Val Loss: 90.1738\n",
      "saved\n",
      "Epoch: 32, Val Loss: 89.6315\n",
      "Epoch: 32, Train Loss: 1.3303, Val Loss: 89.6315\n",
      "saved\n",
      "Epoch: 33, Val Loss: 90.2832\n",
      "Epoch: 33, Train Loss: 1.3244, Val Loss: 90.2832\n",
      "Epoch: 34, Val Loss: 89.6908\n",
      "Epoch: 34, Train Loss: 1.3220, Val Loss: 89.6908\n",
      "Epoch: 35, Val Loss: 89.3947\n",
      "Epoch: 35, Train Loss: 1.3144, Val Loss: 89.3947\n",
      "saved\n",
      "Epoch: 36, Val Loss: 89.4609\n",
      "Epoch: 36, Train Loss: 1.3074, Val Loss: 89.4609\n",
      "Epoch: 37, Val Loss: 90.0691\n",
      "Epoch: 37, Train Loss: 1.3031, Val Loss: 90.0691\n",
      "Epoch: 38, Val Loss: 89.4366\n",
      "Epoch: 38, Train Loss: 1.3032, Val Loss: 89.4366\n",
      "Epoch: 39, Val Loss: 89.6272\n",
      "Epoch: 39, Train Loss: 1.2968, Val Loss: 89.6272\n",
      "Epoch: 40, Val Loss: 89.6140\n",
      "Epoch: 40, Train Loss: 1.2916, Val Loss: 89.6140\n",
      "Epoch: 41, Val Loss: 89.4970\n",
      "Epoch: 41, Train Loss: 1.2895, Val Loss: 89.4970\n",
      "Epoch: 42, Val Loss: 89.4792\n",
      "Epoch: 42, Train Loss: 1.2838, Val Loss: 89.4792\n",
      "Epoch: 43, Val Loss: 89.1996\n",
      "Epoch: 43, Train Loss: 1.2792, Val Loss: 89.1996\n",
      "saved\n",
      "Epoch: 44, Val Loss: 89.4165\n",
      "Epoch: 44, Train Loss: 1.2739, Val Loss: 89.4165\n",
      "Epoch: 45, Val Loss: 89.2183\n",
      "Epoch: 45, Train Loss: 1.2721, Val Loss: 89.2183\n",
      "Epoch: 46, Val Loss: 89.3163\n",
      "Epoch: 46, Train Loss: 1.2681, Val Loss: 89.3163\n",
      "Epoch: 47, Val Loss: 89.2917\n",
      "Epoch: 47, Train Loss: 1.2641, Val Loss: 89.2917\n",
      "Epoch: 48, Val Loss: 89.2199\n",
      "Epoch: 48, Train Loss: 1.2605, Val Loss: 89.2199\n",
      "Epoch: 49, Val Loss: 89.1545\n",
      "Epoch: 49, Train Loss: 1.2557, Val Loss: 89.1545\n",
      "saved\n",
      "Epoch: 50, Val Loss: 89.3274\n",
      "Epoch: 50, Train Loss: 1.2533, Val Loss: 89.3274\n",
      "Epoch: 51, Val Loss: 89.0941\n",
      "Epoch: 51, Train Loss: 1.2490, Val Loss: 89.0941\n",
      "saved\n",
      "Epoch: 52, Val Loss: 89.5990\n",
      "Epoch: 52, Train Loss: 1.2444, Val Loss: 89.5990\n",
      "Epoch: 53, Val Loss: 89.2759\n",
      "Epoch: 53, Train Loss: 1.2423, Val Loss: 89.2759\n",
      "Epoch: 54, Val Loss: 89.7089\n",
      "Epoch: 54, Train Loss: 1.2390, Val Loss: 89.7089\n",
      "Epoch: 55, Val Loss: 89.8047\n",
      "Epoch: 55, Train Loss: 1.2338, Val Loss: 89.8047\n",
      "Epoch: 56, Val Loss: 89.5441\n",
      "Epoch: 56, Train Loss: 1.2336, Val Loss: 89.5441\n",
      "Epoch: 57, Val Loss: 89.4093\n",
      "Epoch: 57, Train Loss: 1.2290, Val Loss: 89.4093\n",
      "Epoch: 58, Val Loss: 90.0564\n",
      "Epoch: 58, Train Loss: 1.2203, Val Loss: 90.0564\n",
      "Epoch: 59, Val Loss: 89.7706\n",
      "Epoch: 59, Train Loss: 1.2243, Val Loss: 89.7706\n",
      "Epoch: 60, Val Loss: 90.2136\n",
      "Epoch: 60, Train Loss: 1.2170, Val Loss: 90.2136\n",
      "Epoch: 61, Val Loss: 89.6128\n",
      "Epoch: 61, Train Loss: 1.2133, Val Loss: 89.6128\n",
      "Epoch: 62, Val Loss: 89.9014\n",
      "Epoch: 62, Train Loss: 1.2098, Val Loss: 89.9014\n",
      "Epoch: 63, Val Loss: 90.0484\n",
      "Epoch: 63, Train Loss: 1.2051, Val Loss: 90.0484\n",
      "Epoch: 64, Val Loss: 90.4735\n",
      "Epoch: 64, Train Loss: 1.2024, Val Loss: 90.4735\n",
      "Epoch: 65, Val Loss: 90.3786\n",
      "Epoch: 65, Train Loss: 1.2029, Val Loss: 90.3786\n",
      "Epoch: 66, Val Loss: 90.2910\n",
      "Epoch: 66, Train Loss: 1.1953, Val Loss: 90.2910\n",
      "Epoch: 67, Val Loss: 90.5963\n",
      "Epoch: 67, Train Loss: 1.1918, Val Loss: 90.5963\n",
      "Epoch: 68, Val Loss: 90.1195\n",
      "Epoch: 68, Train Loss: 1.1876, Val Loss: 90.1195\n",
      "Epoch: 69, Val Loss: 90.5306\n",
      "Epoch: 69, Train Loss: 1.1830, Val Loss: 90.5306\n",
      "Epoch: 70, Val Loss: 90.6378\n",
      "Epoch: 70, Train Loss: 1.1797, Val Loss: 90.6378\n",
      "Epoch: 71, Val Loss: 90.6884\n",
      "Epoch: 71, Train Loss: 1.1787, Val Loss: 90.6884\n",
      "Epoch: 72, Val Loss: 90.7096\n",
      "Epoch: 72, Train Loss: 1.1735, Val Loss: 90.7096\n"
     ]
    }
   ],
   "source": [
    "result_dir = 'results/'\n",
    "os.makedirs(result_dir, exist_ok=True)\n",
    "train_loss, val_loss = training(\n",
    "    name=result_dir + name, \n",
    "    max_patience=max_patience, \n",
    "    num_epochs=num_epochs, \n",
    "    model=model, \n",
    "    optimizer=optimizer,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
